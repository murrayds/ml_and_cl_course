{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dakota Murray\n",
    "## Applying ML to CL\n",
    "### Assignment 2\n",
    "\n",
    "\n",
    "In this assignment, I use Keras to implement an artificial neural network for Parts of Speech (PoS) tagging and evaluate the result. I use data from the Penn treebank corpus in the NLTK library. I draw heavily from a tutorial at the following link, but make alterations to the code in the hopes of improving performance and better understanding the methodology. \n",
    "\n",
    "https://becominghuman.ai/part-of-speech-tagging-tutorial-with-the-keras-deep-learning-library-d7f93fa05537\n",
    "\n",
    "\n",
    "I use a standard **sequential Neural Network architecture**, rather than something fancy like a recurrent neural net. I feal that, for a first attempt at implementing a PoS tagger that this basic architecture is preferable. \n",
    "\n",
    "It seemed that the Schmitt paper only used the term itself and its suffix to determine the parts of speech. I used these features, but also considered many more. \n",
    "\n",
    "- The term itself\n",
    "- Number of terms in the sentence\n",
    "- Whether the term was the first in the sentence\n",
    "- Whether the term was the last in the sentence\n",
    "- Whether the term was capitalized (i.e.: bill vs. Bill)\n",
    "- Whether the word was all capitalized (i.e.: HELLO)\n",
    "- Whether the word all lowercase (i.e.: hello)\n",
    "- The 1, 2, 3, 4, and 5-character prefix (if long enough) was available\n",
    "- The 1, 2, 3, 4, and 5-character suffic (if long enough) was available\n",
    "- Whether the term was all alphanumeric (i.e.: had no puntuation)\n",
    "- The token before the term (if not the first term)\n",
    "- The token that appears after the term (if not the last term)\n",
    "- Whether the term has a hyphen (as in \"-\")\n",
    "- Whether the term has a period (e.g., U.S.A, Dr.)\n",
    "- Whether the term has an aporstrophe (e.g., \"Jane Smith's\")\n",
    "- whether the term was entirely numeric\n",
    "- Whether the term had any capitalization inside the text (e.g., PhD)\n",
    "\n",
    "\n",
    "Overall, I have included way more features than those used in the Schmitt paper. I am hoping that these new features will mosly help in certain special cases which Schmitt originally didn't especially account for, such as apostrophes denoting proper nouns.\n",
    "\n",
    "I experimneted with many different settings for the neural network classifier. The network I settled on had an input layer, an output layer, and a single hidden layer. I experimented with adding a second hidden layer, but didn't notice any increase in efficiency and so it wasn't worth the training slowdown. I used 512 nodes in the hidden layer; this is what the tutorial used and I didn't notice much difference using other values, and so I kept this as is. I was worried about overfitting, and so I set the \"dropout\" pretty high. Dropout is a technique where some portion of nodes are randomly ignored during each stage of training—this helps distribute waeights to other nodes and makes the architecture less likely to overfit. Rather than sigmoud activation that was used in Schmitt's paper, I use relu activiation, mostly because this seems to be the most-oftenly used in the tutorials and descriptions I read through.\n",
    "\n",
    "For evaluation, I used standard accuracy, as this is what was used in the Schmitt paper and so allows my results to be comparable. \n",
    "\n",
    "My final result is an accuracy on the test set of about 96.3%, which is comparable to that in the Schmitt paper. I guess that this means that all of my extra features didn't really help much, and that really the term + suffix is the most important feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sentence_to_features(sentence_terms, index):\n",
    "    \"\"\" \n",
    "        This function takes as input a list of words \n",
    "        that represent a sentence, and outputs a \n",
    "        dictionary contianing the features to use for \n",
    "        parts of speech tagging\n",
    "    \"\"\"\n",
    "    term = sentence_terms[index]\n",
    "    return {\n",
    "        'nb_terms': len(sentence_terms),\n",
    "        'term': term,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence_terms) - 1,\n",
    "        'is_capitalized': term[0].upper() == term[0],\n",
    "        'is_all_caps': term.upper() == term,\n",
    "        'is_all_lower': term.lower() == term,\n",
    "        'prefix-1': term[0],\n",
    "        'prefix-2': term[:2],\n",
    "        'prefix-3': term[:3],\n",
    "        'prefix-4': term[:4],\n",
    "        'prefix-5': term[:5],\n",
    "        'suffix-1': term[-1],\n",
    "        'suffix-2': term[-2:],\n",
    "        'suffix-3': term[-3:],\n",
    "        'suffix-4': term[-4:],\n",
    "        'suffix-5': term[-5:],\n",
    "        'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence_terms[index])))),\n",
    "        'prev_word': '' if index == 0 else sentence_terms[index - 1],\n",
    "        'next_word': '' if index == len(sentence_terms) - 1 else sentence_terms[index + 1],\n",
    "        'has_hyphen': \"'\" in sentence_terms[index],\n",
    "        'has_period': \".\" in sentence_terms[index],\n",
    "        'has_apost': term.replace(\"'\", \"\") != term,\n",
    "        'is_numeric': sentence_terms[index].isdigit(),\n",
    "        'capitals_inside': sentence_terms[index][1:].lower() != sentence_terms[index][1:]\n",
    "    }\n",
    "\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    \"\"\" \n",
    "    Helper function to remove the tag for each tagged term.\n",
    "    \"\"\"\n",
    "    return [w for w, _ in tagged_sentence]\n",
    "\n",
    "\n",
    "def corpus_to_data(tagged_sentences):\n",
    "    \"\"\"\n",
    "    Given a corpus of PoS tagged sentences, produce a feature dataset\n",
    "    with associated features, X, and output data, y\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    # iterate through every sentence and produce features\n",
    "    for pos_tags in tagged_sentences:\n",
    "        for index, (term, tag) in enumerate(pos_tags):\n",
    "            # Create the features\n",
    "            X.append(sentence_to_features(untag(pos_tags), index))\n",
    "            y.append(tag)\n",
    "            \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the data and produce a training, testing, and validation corpuses. \n",
    "\n",
    "The training data is used to train the neural network model. I am using 80% of the total tagged dataset in order to train the model. \n",
    "\n",
    "I use the remining 20% of the dataset as the testing data, which I use to produce an accuracy score at the end. \n",
    "\n",
    "The validation data is used during training. Basically, this validation data is used to calculate loss of the model and tune the neural net weights. I use 25% of the training data as the validation set, which is then excluded from the remainder of the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset for training and testing\n",
    "from nltk.corpus import treebank\n",
    "sentences = treebank.tagged_sents(tagset='universal')\n",
    "\n",
    "train_test_cutoff = int(.80 * len(sentences)) \n",
    "training_sentences = sentences[:train_test_cutoff]\n",
    "testing_sentences = sentences[train_test_cutoff:]\n",
    "\n",
    "train_val_cutoff = int(.25 * len(training_sentences))\n",
    "validation_sentences = training_sentences[:train_val_cutoff]\n",
    "training_sentences = training_sentences[train_val_cutoff:]\n",
    "\n",
    "# Build the feature sets for each partition\n",
    "X_train, y_train = corpus_to_data(training_sentences)\n",
    "X_test, y_test = corpus_to_data(testing_sentences)\n",
    "X_val, y_val = corpus_to_data(testing_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras only likes its data to be a numeric, vectorized form. However, up until now, the features I created are in the form of a dictionary of numbers and tokens. I need a way to turn these dictionaries into vectors.\n",
    "\n",
    "Fortunately, sklearn comes with some pretty great tools for vectorization. using sklearn's DictVectorizer, I produced a vectorspace that can fit the entire training and testing sets. The vectorizer is fit using all of the data to ensure that they share a common vector space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we need to vectorize our inputs...\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Fit our DictVectorizer with our set of features\n",
    "dict_vectorizer = DictVectorizer(sparse=False)\n",
    "dict_vectorizer.fit(X_train + X_test + X_val)\n",
    "\n",
    "# Convert dict features to vectors\n",
    "X_train = dict_vectorizer.transform(X_train)\n",
    "X_test = dict_vectorizer.transform(X_test)\n",
    "X_val = dict_vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at one of these, and we see that it is just a big numeric matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we also need to encode the labels (the classification tags). This is done using one-hot encoding. There are 12 total tags, so each vector is of size 12. Fortuantely, sklearn also has a good function for doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we encode ouor output vector, y\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fit LabelEncoder with our list of classes\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train + y_test + y_val)\n",
    "\n",
    "# Encode class values as integers\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "y_val = label_encoder.transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use a Keras utility in order create the dummy variables for the classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert integers to dummy variables (one hot encoded)\n",
    "# Use keras module to make it happen\n",
    "from keras.utils import np_utils\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "y_val = np_utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can look at the data and see a series of numeric vectors with a single \"1\" and filled with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a function is created which will create the keras model given a set of input parameters. This function is used in the actual model building below. This is also where I define the architecture of the Keras model classifeir. As stated above, I use a model with a single hidden layer, and use relu activation for each layer, and softmax for the final output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "def build_model(input_dim, hidden_neurons, output_dim):\n",
    "    \"\"\"\n",
    "    This function takes a set of arguments as input and outputs the compuled \n",
    "    (but not trained) Keras model. \n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(hidden_neurons, input_dim=input_dim),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.40),\n",
    "        Dense(hidden_neurons),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.40),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I format a set of data that is used as the model parameters for the Keras classifier. I tried experimenting with several different parameters, but they had a trivial impact on the final accuracy, so I mosly settled on what was provided by the main tutorial I was following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "model_params = {\n",
    "    'build_fn': build_model,\n",
    "    'input_dim': X_train.shape[1],\n",
    "    'hidden_neurons': 512,\n",
    "    'output_dim': y_train.shape[1],\n",
    "    'epochs': 5,\n",
    "    'batch_size': 256,\n",
    "    'verbose': 1,\n",
    "    'validation_data': (X_val, y_val),\n",
    "    'shuffle': True\n",
    "}\n",
    "\n",
    "clf = KerasClassifier(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I actually do the training. The history of the training is shown—it seems like the accuracy mosly levels off after the second epoch of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61107 samples, validate on 20039 samples\n",
      "Epoch 1/5\n",
      "61107/61107 [==============================] - 249s 4ms/step - loss: 0.3776 - acc: 0.8902 - val_loss: 0.0973 - val_acc: 0.9675\n",
      "Epoch 2/5\n",
      "61107/61107 [==============================] - 237s 4ms/step - loss: 0.0784 - acc: 0.9746 - val_loss: 0.0880 - val_acc: 0.9703\n",
      "Epoch 3/5\n",
      "61107/61107 [==============================] - 225s 4ms/step - loss: 0.0534 - acc: 0.9827 - val_loss: 0.1014 - val_acc: 0.9672\n",
      "Epoch 4/5\n",
      "61107/61107 [==============================] - 234s 4ms/step - loss: 0.0416 - acc: 0.9858 - val_loss: 0.1004 - val_acc: 0.9646\n",
      "Epoch 5/5\n",
      "61107/61107 [==============================] - 240s 4ms/step - loss: 0.0344 - acc: 0.9882 - val_loss: 0.1110 - val_acc: 0.9633\n"
     ]
    }
   ],
   "source": [
    "hist = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I print the accuracy of the model, which is just the accuracy of the trained model's performance at classifying the test data. The final score is very similar to that reported by the Schmitt paper, so all this facny training didn't seem to improve that much over the basic original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20039/20039 [==============================] - 22s 1ms/step\n",
      "0.9633215230300913\n"
     ]
    }
   ],
   "source": [
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
